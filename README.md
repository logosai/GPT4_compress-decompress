# GPT4_compress-decompress
This repo demostrates how GPT-4 (ChatGPT_5th_of_April_2023) compresses code into its internal language which in turn is used as a source for it (in another chat instance) to deconstruct the original code.
Science here demonstrates how the intention was preserved with code formally deformed.

Addition:
GPT-4 estimated savings of tokens: "The original code consisted of 189 tokens, while my compressed code took 104 tokens."
